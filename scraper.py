import requests
import json
import xml.etree.ElementTree as ET
import datetime
import os
import time
import arxiv 

API_KEY = os.environ.get("DEEPSEEK_API_KEY")

# --- 1. å¹¿æ’’ç½‘ï¼šGoogle News æº ---
RSS_SOURCES = [
    {
        "tag": "CNÂ·è¡Œä¸š",
        "url": "https://news.google.com/rss/search?q=å…·èº«æ™ºèƒ½+OR+äººå½¢æœºå™¨äºº+OR+ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶+OR+ä¸–ç•Œæ¨¡å‹+when:1d&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
    },
    {
        "tag": "CNÂ·ä¼ä¸š",
        "url": "https://news.google.com/rss/search?q=å®‡æ ‘ç§‘æŠ€+OR+æ™ºå…ƒæœºå™¨äºº+OR+åä¸ºADS+OR+ç‰¹æ–¯æ‹‰FSD+OR+Waymo+when:1d&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
    },
    {
        "tag": "ENÂ·Tech",
        "url": "https://news.google.com/rss/search?q=\"Embodied+AI\"+OR+\"Humanoid+Robot\"+OR+\"Tesla+Optimus\"+OR+\"NVIDIA+Isaac\"+when:1d&hl=en-US&gl=US&ceid=US:en"
    },
    {
        "tag": "ENÂ·Auto",
        "url": "https://news.google.com/rss/search?q=\"End-to-end+Autonomous+Driving\"+OR+\"Robotaxi\"+OR+\"Waymo\"+when:1d&hl=en-US&gl=US&ceid=US:en"
    }
]

# --- 2. ç²¾å‡†æ‰“å‡»ï¼šArXiv è®ºæ–‡æº ---
ARXIV_QUERIES = [
    "abs:\"Embodied AI\"", 
    "abs:\"Autonomous Driving\" AND abs:\"End-to-end\"",
    "abs:\"Humanoid Robot\""
]

def fetch_rss(source_config):
    print(f"ğŸ“¡ æ‰«æ RSS: {source_config['tag']} ...")
    try:
        resp = requests.get(source_config['url'], timeout=15)
        root = ET.fromstring(resp.content)
        items = []
        for item in root.findall('./channel/item'):
            title = item.find('title').text
            link = item.find('link').text
            try:
                dt = datetime.datetime.strptime(item.find('pubDate').text[:16], '%a, %d %b %Y')
                date_str = dt.strftime('%Y-%m-%d')
            except:
                date_str = datetime.date.today().strftime('%Y-%m-%d')
            
            source = source_config['tag']
            if "arxiv" in title.lower(): source = "PaperÂ·è®ºæ–‡"
            items.append({"title": title, "link": link, "date": date_str, "source": source, "lang": "CN" if "CN" in source else "EN"})
        return items
    except:
        return []

def fetch_arxiv_papers():
    print("ğŸ“ è¿æ¥ ArXiv å­¦æœ¯åº“...")
    items = []
    try:
        for query in ARXIV_QUERIES:
            search = arxiv.Search(query=query, max_results=5, sort_by=arxiv.SortCriterion.SubmittedDate)
            for result in search.results():
                pub_date = result.published.date()
                if (datetime.date.today() - pub_date).days > 3: continue
                items.append({
                    "title": result.title,
                    "link": result.entry_id,
                    "date": str(pub_date),
                    "source": "PaperÂ·è®ºæ–‡",
                    "lang": "EN"
                })
        return items
    except Exception as e:
        print(f"âŒ ArXiv é”™è¯¯: {e}")
        return []

def call_ai_summary(text, lang):
    if not API_KEY: return "æœªé…ç½® API Key"
    prompt = "ä½ æ˜¯ä¸€åç§‘æŠ€åˆ†æå¸ˆã€‚åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä¸å…·èº«æ™ºèƒ½/è‡ªåŠ¨é©¾é©¶/æœºå™¨äººé«˜åº¦ç›¸å…³ã€‚æ— å…³å›å¤SKIPã€‚ç›¸å…³åˆ™ç”¨ä¸­æ–‡ç”Ÿæˆ80å­—æ·±åº¦è§£è¯»ï¼ˆæ ¸å¿ƒ+æ„ä¹‰ï¼‰ã€‚"
    url = "https://api.deepseek.com/chat/completions"
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "system", "content": prompt}, {"role": "user", "content": f"Title: {text}"}],
        "stream": False
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}
    try:
        res = requests.post(url, headers=headers, json=payload, timeout=20)
        content = res.json()['choices'][0]['message']['content']
        return None if "SKIP" in content else content
    except:
        return None

# --- ä¿®æ”¹é‡ç‚¹ï¼šå¼ºåˆ¶æ³¨å…¥ä»Šå¤©çš„æ—¥æœŸ ---
def generate_daily_brief(today_items):
    if not API_KEY or not today_items: return
    
    # 1. è·å– Python ç®—å‡ºæ¥çš„çœŸå®æ—¥æœŸ
    today_str = datetime.date.today().strftime('%Y-%m-%d')
    
    print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆã€{today_str} æ—¥æŠ¥ã€‘...")
    titles = [item['title'] for item in today_items[:20]]
    titles_text = "\n".join(titles)
    
    # 2. åœ¨æç¤ºè¯é‡Œç›´æ¥å‘Šè¯‰ AI ä»Šå¤©æ˜¯å‡ å·
    prompt = f"""
    ä½ æ˜¯ä¸€åé¡¶çº§è¡Œä¸šåˆ†æå¸ˆã€‚ä»Šå¤©æ˜¯ {today_str}ã€‚
    è¯·æ ¹æ®ä»Šæ—¥æŠ“å–çš„æ–°é—»æ ‡é¢˜ï¼Œå†™ä¸€ç¯‡ã€å…·èº«æ™ºèƒ½ä¸è‡ªåŠ¨é©¾é©¶æ—¥æŠ¥ã€‘ã€‚
    
    ã€æ ¼å¼è¦æ±‚ã€‘ï¼š
    1. ä½¿ç”¨ Markdown æ ¼å¼ã€‚
    2. ç¬¬ä¸€è¡Œå¿…é¡»æ˜¯ï¼š### ğŸ“… è¡Œä¸šè¶‹åŠ¿åˆ†æ ({today_str})
    3. å†…å®¹åŒ…å«ä¸‰ä¸ªæ¿å—ï¼š
       - ğŸš€ **é‡ç‚¹çªå‘**ï¼šä»Šæ—¥æœ€é‡è¦çš„1-2ä»¶äº‹ã€‚
       - ğŸ’¡ **æŠ€æœ¯é£å‘**ï¼šæœ‰ä»€ä¹ˆæ–°æŠ€æœ¯æˆ–è®ºæ–‡å‡ºç°ã€‚
       - ğŸ“Š **å¸‚åœºåŠ¨æ€**ï¼šä¼ä¸šèèµ„æˆ–åˆä½œåŠ¨æ€ã€‚
    4. å­—æ•°æ§åˆ¶åœ¨ 400 å­—ä»¥å†…ï¼Œè¯­è¨€çŠ€åˆ©ï¼Œè§‚ç‚¹é²œæ˜ã€‚
    """
    
    url = "https://api.deepseek.com/chat/completions"
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "system", "content": prompt}, {"role": "user", "content": f"ä»Šæ—¥æ–°é—»:\n{titles_text}"}],
        "stream": False
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}
    
    try:
        res = requests.post(url, headers=headers, json=payload, timeout=60)
        content = res.json()['choices'][0]['message']['content']
        
        with open('daily_brief.json', 'w', encoding='utf-8') as f:
            json.dump({"date": today_str, "content": content}, f, ensure_ascii=False, indent=2)
        print("âœ… æ—¥æŠ¥ç”ŸæˆæˆåŠŸï¼(daily_brief.json)")
    except Exception as e:
        print(f"âŒ æ—¥æŠ¥ç”Ÿæˆå¤±è´¥: {e}")

def job():
    all_items = []
    for source in RSS_SOURCES:
        all_items.extend(fetch_rss(source))
        time.sleep(1)
    
    all_items.extend(fetch_arxiv_papers())

    if os.path.exists('data.json'):
        try:
            with open('data.json', 'r', encoding='utf-8') as f: old_data = json.load(f)
        except: old_data = []
    else: old_data = []

    seen = set(i['title'] for i in old_data)
    final_data = old_data
    
    today_new_items = []

    print(f"ğŸ” åŸå§‹æŠ“å– {len(all_items)} æ¡ï¼Œå¼€å§‹ AI æ¸…æ´—...")
    for item in all_items:
        if item['title'] in seen: continue
        
        summary = call_ai_summary(item['title'], item['lang'])
        if summary: 
            item['summary'] = summary
            final_data.insert(0, item)
            today_new_items.append(item)
            seen.add(item['title'])
            print(f"âœ… æ”¶å½•: {item['title'][:15]}...")
        time.sleep(0.5)

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_data[:600], f, ensure_ascii=False, indent=2)
    
    if len(today_new_items) > 0:
        generate_daily_brief(today_new_items)
    elif len(final_data) > 0:
        generate_daily_brief(final_data[:15])

if __name__ == "__main__":
    job()
