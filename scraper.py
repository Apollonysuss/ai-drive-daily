import requests
import json
import xml.etree.ElementTree as ET
import datetime
import os
import time

API_KEY = os.environ.get("DEEPSEEK_API_KEY")

# --- å‡çº§ç‰ˆæ•°æ®æºé…ç½®ï¼šå¹¿æ’’ç½‘ ---
RSS_SOURCES = [
    # 1. å›½å†…äº§ä¸š/èµ„æœ¬ (è¦†ç›– 36æ°ª, è™å—…, é’›åª’ä½“, æœºå™¨ä¹‹å¿ƒç­‰)
    {
        "tag": "CNÂ·è¡Œä¸š",
        "url": "https://news.google.com/rss/search?q=å…·èº«æ™ºèƒ½+OR+äººå½¢æœºå™¨äºº+OR+ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶+OR+Robotaxi+OR+ä¸–ç•Œæ¨¡å‹+when:1d&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
    },
    {
        "tag": "CNÂ·å…¬å¸",
        "url": "https://news.google.com/rss/search?q=å®‡æ ‘ç§‘æŠ€+OR+æ™ºå…ƒæœºå™¨äºº+OR+åä¸ºADS+OR+å°é¹NGP+OR+ç‰¹æ–¯æ‹‰FSD+OR+FigureAI+when:1d&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
    },
    # 2. å›½é™…å‰æ²¿ (è¦†ç›– TechCrunch, TheVerge, Medium ç­‰)
    {
        "tag": "ENÂ·Tech",
        "url": "https://news.google.com/rss/search?q=\"Embodied+AI\"+OR+\"Humanoid+Robot\"+OR+\"Foundation+Model+for+Robotics\"+OR+\"Sim-to-Real\"+when:1d&hl=en-US&gl=US&ceid=US:en"
    },
    {
        "tag": "ENÂ·Auto",
        "url": "https://news.google.com/rss/search?q=\"End-to-end+Autonomous+Driving\"+OR+\"Waymo\"+OR+\"Tesla+Optimus\"+OR+\"NVIDIA+Isaac\"+when:1d&hl=en-US&gl=US&ceid=US:en"
    },
    # 3. å­¦æœ¯è®ºæ–‡ (è¦†ç›– Arxiv, CVPR, ICLR ç­‰ä¼šè®®ç›¸å…³æŠ¥é“)
    {
        "tag": "PaperÂ·è®ºæ–‡",
        "url": "https://news.google.com/rss/search?q=site:arxiv.org+(\"Embodied+AI\"+OR+\"Autonomous+Driving\"+OR+\"World+Model\"+OR+\"Imitation+Learning\")+when:1d&hl=en-US&gl=US&ceid=US:en"
    }
]

def fetch_rss(source_config):
    print(f"ğŸ“¡ æŠ“å–æº: {source_config['tag']} ...")
    try:
        resp = requests.get(source_config['url'], timeout=15)
        root = ET.fromstring(resp.content)
        items = []
        for item in root.findall('./channel/item'):
            title = item.find('title').text
            link = item.find('link').text
            try:
                dt = datetime.datetime.strptime(item.find('pubDate').text[:16], '%a, %d %b %Y')
                date_str = dt.strftime('%Y-%m-%d')
            except:
                date_str = datetime.date.today().strftime('%Y-%m-%d')
            
            source_name = source_config['tag']
            if "arxiv" in title.lower() or "arxiv" in link.lower():
                source_name = "PaperÂ·è®ºæ–‡"

            items.append({
                "title": title,
                "link": link,
                "date": date_str,
                "source": source_name,
                "lang": "CN" if "CN" in source_config['tag'] else "EN"
            })
        return items
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        return []

def call_ai_summary(text, lang):
    if not API_KEY: return "æœªé…ç½® API Key"
    
    prompt = """
    ä½ æ˜¯ä¸€åç§‘æŠ€æƒ…æŠ¥åˆ†æå¸ˆã€‚è¯·é˜…è¯»æ–°é—»æ ‡é¢˜ï¼Œç”¨ä¸­æ–‡ç”Ÿæˆä¸€æ®µçº¦ 80-100 å­—çš„æ·±åº¦è§£è¯»ã€‚
    æ ¼å¼è¦æ±‚ï¼š
    1. ã€æ ¸å¿ƒå†…å®¹ã€‘ï¼šç®€è¿°å‘ç”Ÿäº†ä»€ä¹ˆã€‚
    2. ã€å…³é”®æ„ä¹‰ã€‘ï¼šå¯¹è¡Œä¸šæ„å‘³ç€ä»€ä¹ˆã€‚
    ä¸è¦ä½¿ç”¨Markdownæ ¼å¼ã€‚
    """
    if "arxiv" in text.lower():
        prompt = "ä½ æ˜¯ä¸€åå­¦æœ¯åŠ©æ‰‹ã€‚è¯·é˜…è¯»è®ºæ–‡æ ‡é¢˜ï¼Œç”¨ä¸­æ–‡ç®€è¿°å…¶ç ”ç©¶æ–¹å‘å’Œæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆ80å­—å·¦å³ï¼‰ã€‚"

    url = "https://api.deepseek.com/chat/completions"
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "system", "content": prompt}, {"role": "user", "content": f"Title: {text}"}],
        "stream": False
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

    try:
        res = requests.post(url, headers=headers, json=payload, timeout=20)
        return res.json()['choices'][0]['message']['content']
    except:
        return "AI åˆ†æè¶…æ—¶"

def job():
    all_new_items = []
    for source in RSS_SOURCES:
        items = fetch_rss(source)
        all_new_items.extend(items[:3])
        time.sleep(1)

    if os.path.exists('data.json'):
        with open('data.json', 'r', encoding='utf-8') as f:
            try: old_data = json.load(f)
            except: old_data = []
