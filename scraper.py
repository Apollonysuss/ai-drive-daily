import requests
import json
import xml.etree.ElementTree as ET
import datetime
import os
import time
import arxiv # éœ€è¦åœ¨ workflow é‡Œ pip install arxiv

API_KEY = os.environ.get("DEEPSEEK_API_KEY")

# --- 1. Google News æ³›æœç´¢æº ---
RSS_SOURCES = [
    {
        "tag": "CNÂ·è¡Œä¸šåŠ¨æ€",
        "url": "https://news.google.com/rss/search?q=å…·èº«æ™ºèƒ½+OR+äººå½¢æœºå™¨äºº+OR+ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶+OR+ä¸–ç•Œæ¨¡å‹+when:1d&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
    },
    {
        "tag": "ENÂ·Global Tech",
        "url": "https://news.google.com/rss/search?q=\"Embodied+AI\"+OR+\"Humanoid+Robot\"+OR+\"Tesla+Optimus\"+OR+\"NVIDIA+Gr00t\"+when:1d&hl=en-US&gl=US&ceid=US:en"
    }
]

# --- 2. ArXiv è®ºæ–‡ç²¾å‡†æºå…³é”®è¯ ---
ARXIV_QUERIES = [
    "abs:\"Embodied AI\"", 
    "abs:\"Autonomous Driving\" AND abs:\"End-to-end\"",
    "abs:\"Humanoid Robot\""
]

def fetch_rss(source_config):
    print(f"ğŸ“¡ æ­£åœ¨æ‰«ææ–°é—»æº: {source_config['tag']} ...")
    try:
        resp = requests.get(source_config['url'], timeout=15)
        root = ET.fromstring(resp.content)
        items = []
        for item in root.findall('./channel/item'):
            title = item.find('title').text
            link = item.find('link').text
            try:
                dt = datetime.datetime.strptime(item.find('pubDate').text[:16], '%a, %d %b %Y')
                date_str = dt.strftime('%Y-%m-%d')
            except:
                date_str = datetime.date.today().strftime('%Y-%m-%d')
            
            # æ¥æºæ ‡ç­¾æ¸…æ´—
            source = source_config['tag']
            if "arxiv" in title.lower(): source = "PaperÂ·è®ºæ–‡"
            
            items.append({
                "title": title, "link": link, "date": date_str, 
                "source": source, "lang": "CN" if "CN" in source else "EN"
            })
        return items
    except Exception as e:
        print(f"âŒ RSSæŠ“å–é”™è¯¯: {e}")
        return []

def fetch_arxiv_papers():
    print("ğŸ“ æ­£åœ¨è¿æ¥ ArXiv å­¦æœ¯æ•°æ®åº“...")
    items = []
    try:
        # æœç´¢æœ€è¿‘æäº¤çš„è®ºæ–‡
        for query in ARXIV_QUERIES:
            search = arxiv.Search(
                query = query,
                max_results = 5, # æ¯ä¸ªè¯æŠ“5ç¯‡ï¼Œä¿è¯å…¨
                sort_by = arxiv.SortCriterion.SubmittedDate
            )
            for result in search.results():
                # åªä¿ç•™æœ€è¿‘2å¤©çš„ï¼Œä¿è¯æ–°é²œ
                published_date = result.published.date()
                if (datetime.date.today() - published_date).days > 2:
                    continue
                    
                items.append({
                    "title": result.title,
                    "link": result.entry_id,
                    "date": str(published_date),
                    "source": "PaperÂ·è®ºæ–‡",
                    "lang": "EN"
                })
        print(f"   -> æŠ“å–åˆ° {len(items)} ç¯‡æ–°è®ºæ–‡")
        return items
    except Exception as e:
        print(f"âŒ ArXiv æ¥å£æŠ¥é”™: {e}")
        return []

def call_ai_summary(text, lang):
    if not API_KEY: return "æœªé…ç½® API Key"
    
    prompt = """
    ä½ æ˜¯ä¸€åç§‘æŠ€æƒ…æŠ¥åˆ†æå¸ˆã€‚è¯·åˆ¤æ–­ä»¥ä¸‹æ ‡é¢˜æ˜¯å¦ä¸â€œå…·èº«æ™ºèƒ½â€æˆ–â€œè‡ªåŠ¨é©¾é©¶â€é«˜åº¦ç›¸å…³ã€‚
    å¦‚æœã€æ— å…³ã€‘ï¼ˆå¦‚å¹¿å‘Šã€è‚¡å¸‚ã€æ— å…³ç¤¾ä¼šæ–°é—»ï¼‰ï¼Œè¯·åªå›å¤â€œSKIPâ€ã€‚
    å¦‚æœã€ç›¸å…³ã€‘ï¼Œè¯·ç”¨ä¸­æ–‡ç”Ÿæˆ80å­—å·¦å³çš„æ·±åº¦è§£è¯»ï¼ˆåŒ…å«æ ¸å¿ƒå†…å®¹+è¡Œä¸šæ„ä¹‰ï¼‰ã€‚
    """
    
    url = "https://api.deepseek.com/chat/completions"
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "system", "content": prompt}, {"role": "user", "content": f"Title: {text}"}],
        "stream": False
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

    try:
        res = requests.post(url, headers=headers, json=payload, timeout=20)
        content = res.json()['choices'][0]['message']['content']
        if "SKIP" in content: return None # AI è®¤ä¸ºæ— å…³ï¼Œè¿‡æ»¤æ‰
        return content
    except:
        return None

def job():
    all_items = []
    
    # 1. æŠ“ RSS æ–°é—»
    for source in RSS_SOURCES:
        all_items.extend(fetch_rss(source))
        time.sleep(1)
        
    # 2. æŠ“ ArXiv è®ºæ–‡
    all_items.extend(fetch_arxiv_papers())

    # 3. è¯»å–æ—§æ•°æ®
    if os.path.exists('data.json'):
        try:
            with open('data.json', 'r', encoding='utf-8') as f:
                old_data = json.load(f)
        except: old_data = []
    else:
        old_data = []

    seen = set(i['title'] for i in old_data)
    final_data = old_data
    
    # 4. AI è¿‡æ»¤ä¸æ€»ç»“
    print(f"ğŸ” åŸå§‹æŠ“å– {len(all_items)} æ¡ï¼Œå¼€å§‹ AI æ™ºèƒ½æ¸…æ´—...")
    new_count = 0
    
    for item in all_items:
        if item['title'] in seen: continue
        
        # è®© AI å†³å®šç•™ä¸ç•™
        summary = call_ai_summary(item['title'], item['lang'])
        if summary: 
            item['summary'] = summary
            final_data.insert(0, item)
            seen.add(item['title'])
            new_count += 1
            print(f"âœ… æ”¶å½•: {item['title'][:15]}...")
        else:
            print(f"ğŸ—‘ï¸ å‰”é™¤æ— å…³: {item['title'][:15]}...")
        
        time.sleep(0.5) # é˜²æ­¢ API è¶…é™

    # ä¿ç•™ 500 æ¡
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_data[:500], f, ensure_ascii=False, indent=2)
    print(f"ğŸ‰ æ›´æ–°å®Œæˆï¼Œç» AI ç­›é€‰åæ–°å…¥åº“ {new_count} æ¡ã€‚")

if __name__ == "__main__":
    job()
